When a search is made on the Internet using a search engine there is first a traditional text processing part, where the aim is to find all the web pages containing the words of the query. Due to the massive size of the Web, the number of hits is likely to be much too large to be handled by the user. Therefore, some measure of quality is needed to filter away pages that are likely to be less interesting. When one uses a web search engine it is typical that the search phrase is under-specified. Example 13.1 A Google22 search conducted on September 29, 2005, using the search phrase university, gave as a result links to the following well-known universities: Harvard, Stanford, Cambridge, Yale, Cornell, Oxford. The total number of web pages relevant to the search phrase was more than 2 billion.

Obviously, Google uses an algorithm for ranking all the web pages that agrees rather well with a common-sense quality measure. Somewhat surprisingly, the ranking procedure is not based on human judgement, but on the link structure of the web. Loosely speaking, Google assigns a high rank to a web page, if it has inlinks from other pages that have a high rank. We will see that this "self-referencing" statement can be formulated mathematically as an eigenvalue equation for a certain matrix.

It is of course impossible to define a generally valid measure of relevance that would be acceptable for a majority of users of a search engine. Google uses the concept of pagerank as a quality measure of web pages. It is based on the assumption that the number of links to and from a page give information about the importance of a page. We will give a description of pagerank based primarily on [61] and [26]. Concerning Google, see [14].

Let all web pages be ordered from 1 to n and let i be a particular web page. Then Oi will denote the set of pages that i is linked to, the outlinks. The number of outlinks is denoted Ni = |Oi|. The set of inlinks, denoted Ii, are the pages that have an outlink to i.

In general, a page i can be considered as more important the more inlinks it has. However, a ranking system based only on the number of inlinks is easy to manipulate [23]: When you design a web page i that (e.g. for commercial reasons) you would like to be seen by as many as possible, you could simply create a large number of (information-less and unimportant) pages that have outlinks to i. In order to discourage this, one defines the rank of i in such a way that if a highly ranked page j, has an outlink to i, this adds to the importance of i in the following way: the rank of page i is a weighted sum of the ranks of the pages that have outlinks to i. The weighting is such that the rank of a page j is divided evenly among its outlinks. The preliminary definition of Pagerank is ri = X.

This definition is recursive, so pageranks cannot be computed directly. Instead a fixed-point iteration might be used: Guess an initial ranking vector r0.

There are a few problems with this iteration: if a page has no outlinks, then in the iteration process it only accumulates rank via its inlinks, but this rank is never distributed further. Therefore, it is not clear if the iteration converges. We will come back to this question later.

For an example of attempts to fool a search engine, see [78].

More insight is gained if we reformulate (13.1) as an eigenvalue problem fora matrix representing the graph of the Internet. Let Q be a square matrix of dimension n. Then define Qij = ae 1/Nj if there is a link from j to i0 otherwise. This definition means that row i has nonzero elements in those positions that correspond to inlinks of i. Similarly, column j has nonzero elements equal to Nj in those positions that correspond to the outlinks of j, and, provided that the page has outlinks, the sum of all the elements in column j is equal to one. In the following symbolic picture of the matrix Q, non-zero elements are denoted *:

Since page 4 has no outlinks, the corresponding column is equal to zero.

Obviously, the definition (13.1) is equivalent to the scalar product of row i and the vector r, which holds the ranks of all pages. We can write the equation in matrix form: *r = Qr, * = 1, (13.3) i.e. r is an eigenvector of Q with eigenvalue * = 1. It is now easily seen that the iteration (13.2) is equivalent to r(k+1) = Qr(k), k = 0, 1, . . ., which is the power method for computing the eigenvector. However, at this point it is not clear that pagerank is well-defined, as we do not know if there exists an eigenvalue equal to 1. It turns out that the theory of Markov chains is useful in the analysis.

There is a random walk interpretation of the pagerank concept. Assume that a surfer visiting a web page, chooses the next page among the outlinks with equal probability. Then the random walk induces a Markov chain, see e.g. [59, 52].

A Markov chain is a random process, where the next state is determined completely from the present state; the process has no memory [59].

The transition matrix of the Markov chain is QT. (Note that we use a slightly different notation than is common in the theory of stochastic processes).

The random surfer should never get stuck. In other words, our random walk model should have no pages without outlinks (such a page corresponds to a zero column in Q). Therefore, the model is modified so that zero columns are replaced by a constant value in all positions. This means that there is equal probability to go to any other page in the net.

The modified matrix is defined P = Q + 1n edT.

With this modification the matrix P is a proper column-stochastic matrix: It has non-negative elements and the elements of each column sum up to 1. The preceding statement can be reformulated as follows.

Proposition 13.3. A column-stochastic matrix P satisfies eT P = eT, (13.6) where e is defined by (13.4).

Now, in analogy to (13.3), we would like to define the pagerank vector as a unique eigenvector of P with eigenvalue 1, P r = r. However, the existence of such a unique eigenvalue is still not guaranteed. The eigenvector of the transition matrix corresponds to a stationary probability distribution for the Markov chain: The element in position i, ri, is the probability that after a large number of steps, the random walker is at web page i. To ensure the existence of a unique distribution, the matrix must be irreducible, cf. [46].

Definition 13.5. A square matrix A is called reducible if there is a permutation matrix P such that P AP T = `X Y0 Zâ€™, (13.7) where X and Z are both square. Otherwise the matrix is called irreducible.

Example 13.6 To illustrate the concept of reducibility, we give an example of a link graph that corresponds to a reducible matrix.

A random walker who has entered the left part of the link graph will never get out of it, and similarly he will get stuck in the right part. The corresponding matrix is which is of the form. Actually, this matrix has two eigenvalues equal to 1, and one equal to -1, see Example 13.10 below.

The directed graph corresponding to an irreducible matrix is strongly connected: given any two nodes (Ni, Nj), in the graph, there exists a path leading from Ni to Nj.

The uniqueness of the largest eigenvalue of an irreducible matrix is guaranteed by the Perron-Frobenius theorem; we state it for the special case treated here. The inequality A > 0 is understood as all the elements of A being strictly positive. Theorem 13.7. Let A be an irreducible column-stochastic matrix. The largest eigenvalue in magnitude is equal to 1. There is a unique corresponding eigenvector r satisfying r > 0, and krk1 = 1; this is the only eigenvector that is non-negative.

If A > 0, then |*i| < 1, i = 2, 3, . . . , n.

Proof. Due to the fact that A is column-stochastic we have eT A = eT, which means that 1 is an eigenvalue of A. The rest of the statement can be proved using Perron-Frobenius theory [59, Chapter 8].

Given the size of the Internet, we can be sure that the link matrix P is reducible, which means that the pagerank eigenvector of P is not well-defined. To ensure irreducibility, i.e. to make it impossible for the random walker to get trapped in a subgraph, one adds, artificially, a link from every web page to all the other. In matrix terms, this can be made by taking a convex combination of P and a rank one matrix, A = ffP + (1 - ff) 1n eeT , (13.9) for some ff satisfying 0 <= ff <= 1. It is easy to see that the matrix A is column-stochastic: eT A = ffeT P + (1 - ff) 1n eT eeT = ffeT + (1 - ff)eT = eT.

The random walk interpretation of the additional rank one term is that in each timestep the surfer visiting a page will jump to a random page with probability 1 - ff (sometimes referred to as teleportation).

We now see that the pagerank vector for the matrix A is well-defined.

The column-stochastic matrix A defined in (13.8) is irreducible (since A > 0) and has the largest in magnitude eigenvalue * = 1. The corresponding eigenvector r satisfies r > 0.

For the convergence of the numerical eigenvalue algorithm, it is essential to know, how the eigenvalues of P are changed by the rank one modification (13.8).

Theorem 13.9 ([52]). Assume that the eigenvalues of the column-stochastic matrix P are {1, *2, *3 . . ., *n}. Then the eigenvalues of A = ffP + (1 - ff) 1n eeT are {1, ff*2, ff*3, . . ., ff*n}.

Proof. Define ^e to be e normalized to Euclidean length 1 and let U1 2 Rn*(n-1) be such that U = \Gamma ^e U1\Delta is orthogonal. Then, since ^eT P = ^eT, where w = U T1 P ^e, and T = U T1 P T U1. Since we have made a similarity transformation, the matrix T has the eigenvalues *2, *3, . . ., *n.

The statement now follows immediately.

Theorem 13.9 implies that even if P has a multiple eigenvalue equal to 1, which is actually the case for the Google matrix, the second largest eigenvalue in magnitude of A is always equal to ff. Example 13.10 We compute the eigenvalues and eigenvectors of the matrix A = ffP + (1 - ff) 1n eeT , with P from (13.7) and ff = 0.85. The MATLAB code

Page Ranking for a Web Search Engine gives the following result:

It is seen that all other eigenvectors except the first one (which corresponds to the eigenvalue 1), have both positive and negative components, as stated in Theorem 13.7.

Instead of the modification (13.8) we can define A = ffP + (1 - ff)veT , where v is a non-negative vector with k v k1 = 1 that can be chosen in order to make the search biased towards certain kinds of web pages. Therefore, it is often referred to as a personalization vector [61, 40]. The vector v can also be used for avoiding manipulation by so called link farms [52].

We want to solve the eigenvalue problem Ar = r, where r is normalized k r k1 = 1. In this section we denote the sought eigenvector by r1. In dealing with stochastic matrices and vectors that are probability distributions, it is natural to use the 1-norm for vectors (Section 2.3). Due to the sparsity and the dimension of A (estimated to be of the order billions), it is out of the question to compute the eigenvector using any of the standard methods for dense matrices that are based on applying orthogonal transformations to the matrix described in Chapter 16. The only viable method so far is the power method.

Assume that an initial approximation r(0) is given. The power method is given in the following algorithm.

The power method for Ar = *r for k = 1, 2, . . . until convergence q(k) = Ar(k-1) r(k) = q(k)/k q(k) k1

The purpose of normalizing the vector (making it have 1-norm equal to 1) is to avoid that the vector becomes either very large of very small, and thus unrepresentable in the floating-point system. We will see later that normalization is not necessary in the pagerank computation.

In this context there is no need to compute an eigenvalue approximation, as the eigenvalue sought for is known to be equal to one. The convergence of the power method depends on the distribution of eigenvalues. In order to make the presentation simpler, we assume that A is diagonalizable, i.e. there exists a non-singular matrix R of eigenvectors, R-1AR = diag(*1, . . . , *n).The eigenvalues *i are ordered 1 = *1 > |*2| >= * * * >= |*n|. Expand the initial ap-proximation r(0) in terms of the eigenvectors,

Obviously, since for j = 2, 3, . . . , we have |*j| < 1, the second term tends to zero and the power method converges to the eigenvector r1. The rate of convergence is determined by the ratio | *2/*1|. If this ratio is close to 1, then the iteration is very slow. Fortunately, this is not the case for the Google matrix, see Theorem 13.9 and below. A stopping criterion for the power iteration can be formulated in terms of the residual vector for the eigenvalue problem. Let ^* be the computed approximation of the eigenvalue, and ^r1 the corresponding approximate eigenvector. Then it can be shown [76], [4, p. 229] that the optimal error matrix E, for which (A + E)^r1 = ^*^r1, exactly, satisfies k E k2 = k s k2, where s = A^r1 - ^*^r1. This means that if the residual ksk2 is small, then the computed approximate eigenvector ^r1 is the exact eigenvector of a matrix A + E that is close to A. In the case of pagerank computations it is natural to use the1-norm instead [48], which does not make much difference, since the norms are equivalent (2.5). In view of the huge dimension of the Google matrix, it is non-trivial to compute the matrix-vector product y = Az, where A = ffP + (1 - ff) 1n eeT. First, we show that normalization of the vectors produced in the power iteration is unnecessary.

This assumption can be expected to be satisfied in floating point arithmetic, if not at the first iteration, so after the second, due to round off.

Assume that the vector z satisfies k z k1 = eT z = 1, and that the matrix A is column-stochastic. 

Proof. Put y = Az. Then k y k1 = eT y = eT Az = eT z = 1 since A is column-stochastic (eT A = eT).

Then recall that P was constructed from the actual link matrix Q as P = Q + 1n edT , where the row vector d has an element 1 in all those positions that correspond to web pages with no outlinks, see (13.5). This means that to form P, we insert a large number of full vectors in Q, each of the same dimension as the total number of web pages. Consequently, we cannot afford to store P explicitly. Let us look at the multiplication y = Az in some more detail: y = ff(Q + 1n edT)z + (1 - ff)n e(eT z) = ffQz + fi 1n e, (13.13) where fi = ffdT z + (1 - ff)eT z.

However, we do not need to compute fi from this equation. Instead we can use (13.11) in combination with (13.12): 1 = eT (ffQz) + fieT (1n e) = eT (ffQz) + fi. Thus, we have fi = 1 - k ffQz k1. An extra bonus is that we do not use the vector d at all, i.e., we need not know which pages lack outlinks. The following MATLAB code implements the matrix vector multiplication.

Here v = (1/n) e or a personalized teleportation vector, see p. 134. In order to save memory, we even should avoid using the extra vector yhat and replace it by y. From Theorem 13.9 we know that the second eigenvalue of the Google matrix satisfies *2 = ff. A typical value of ff is 0.85. Approximately k = 57 iterations are needed to make the factor 0.85k equal to 10-4. This is reported [52] to be close the number of iterations used by Google.
Figure 13.1. A 20000 * 20000 submatrix of the stanford.edu matrix. Example 13.12 As an example we used the matrix P obtained from the domain stanford.edu25. The number of pages is 281903, and the total number of links is2312497. Part of the matrix is displayed in Figure 13.1. We computed the pagerank vector using the power method with ff = 0.85 and iterated 63 times until the1-norm of the residual was smaller than 10-6. The residual and the final pagerank vector are illustrated in Figure 13.2.

If one computes the pagerank for a subset of the Internet, one particular domain, say, then the matrix P may be of a dimension for which one can use other methods than the power method. In such cases it may be sufficient to use the MATLAB function eigs, which computes a small number of eigenvalues and the corresponding eigenvectors of a sparse matrix using a variant of the Arnoldi (or Lanczos) method, cf. Section 16.7. In view of the fact that one pagerank calculation can take several days, several enhancements of the iteration procedure have been proposed. In [46] an adaptive method is described that checks the convergence of the components of the PageRank vector and avoids performing the power iteration for those components. Up to 30 % speed up has been reported. The block structure of the web is used in [47], and speedups of a factor 2 have been reported. An acceleration method based on Aitken extrapolation is described in [48]. Aggregation methods are discussed in several papers by Langville and Meyer and in [44], and the Arnoldi method in [33].

A variant of Pagerank is proposed in [36]. Further properties of the PageRank matrix are given in [68].

HITS Another method based on the link structure of the web was introduced at the same time as Pagerank [49]. It is called HITS (Hypertext Induced Topic Search), and is based on the concepts of authorities and hubs. An authority is a web page with several inlinks, and a hub has several outlinks. The basic idea is good hubs point to good authorities and good authorities are pointed to by good hubs. Each web page is assigned both a hub score y and an authority score x. Let L be the adjacency matrix of the directed web graph. Then two equations are given that mathematically define the relation between the two scores, based on the basic idea:

The algorithm for computing the scores is the power method, which converges to the left and right singular vectors corresponding to the largest singular value of L. In the implementation of HITS it is not the adjacency matrix of the whole web that is used, but of all the pages relevant to the query. There is now an extensive literature on Pagerank, HITS and other ranking methods. For overviews, see [50, 51, 7]. A combination of HITS and Pagerank has been proposed in [55].

Obviously, the ideas underlying Pagerank and HITS are not restricted to web applications but can be applied to other network analyses. For instance, a variant of the HITS method was recently used in a study of Supreme Court Precedent [29]. A generalization of HITS is given in [13], which also treats synonym extraction.
